{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import sys\n",
    "import os\n",
    "import preprocessor as p\n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from helpers.text_preprocessor import clean_tweets\n",
    "from helpers.data_validation import clean_and_validate_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection and preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have collected data using Twitter API, from older research and projects that discussed depression and anxiety detection, and from the Reddit site. The data collection process was iterable with the process cycle of the project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1 of the data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first cycle, we gathered data that have 20000 rows and then trained the first version of the models using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20007/20007 [00:28<00:00, 699.22it/s] \n"
     ]
    }
   ],
   "source": [
    "data_v1 = pd.read_csv('../data/raw/Mental_Health_Twitter.csv', usecols=['post_text', 'label'])\n",
    "\n",
    "cleaned_tweets = []\n",
    "\n",
    "#loop on every tweet\n",
    "for i in tqdm(range(len(data_v1['post_text']))):\n",
    "    tweet = data_v1['post_text'][i]\n",
    "\n",
    "    #Use tweet-preprocessor module\n",
    "    clean_text = p.clean(tweet) \n",
    "    \n",
    "    #Use clean_tweets function\n",
    "    filtered_tweet = clean_tweets(clean_text) \n",
    "    cleaned_tweets.append(filtered_tweet)\n",
    "\n",
    "data_v1['cleaned_text'] = cleaned_tweets\n",
    "\n",
    "#Select the columns we interested in for classification\n",
    "data_v1 = data_v1[['cleaned_text','label']] \n",
    "\n",
    "#Rename the columns for clarification\n",
    "data_v1.rename(columns = {'cleaned_text':'filtered_tweet',\n",
    "                          'label':'is_depression'}, \n",
    "                          inplace = True) \n",
    "\n",
    "data_v1.to_csv('../data/processed/Mental_Health_Twitter_Processed_v1.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2 of the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collected more data for serving the models and increasing the performance. In the second cycle, we gathered data that have 47185 rows and then trained the second version of the models using it and the first version of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47185/47185 [01:35<00:00, 495.38it/s] \n"
     ]
    }
   ],
   "source": [
    "data_v2 = pd.read_csv('../data/raw/Mental_Health_Dataset.csv')\n",
    "\n",
    "cleaned_tweets = []\n",
    "for i in tqdm(range(len(data_v2['filtered_tweet']))):\n",
    "    tweet = data_v2['filtered_tweet'][i]\n",
    "    clean_text = p.clean(tweet)\n",
    "    filtered_tweet = clean_tweets(clean_text)\n",
    "    cleaned_tweets.append(filtered_tweet)\n",
    "    \n",
    "data_v2['filtered_tweet'] = cleaned_tweets\n",
    "data_v2.to_csv('../data/processed/Mental_Health_Dataset_Processed_v2.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 3 of the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeated the data collection process to get higher performance for our models. In the last cycle, we gathered data that have 232074 rows and then trained the last version of the models using it and the other versions of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233403/233403 [13:37<00:00, 285.37it/s] \n"
     ]
    }
   ],
   "source": [
    "data_v3 = pd.read_csv('../data/raw/Suicide_Detection.csv', encoding = \"ISO-8859-1\", usecols=['text', 'class'])\n",
    "data_v3.rename(columns = {'text':'filtered_tweet',\n",
    "                          'class':'is_depression'},\n",
    "                          inplace = True )\n",
    "\n",
    "data_v3.replace({\"is_depression\": {\"suicide\": 1, \"non-suicide\": 0}}, inplace=True)\n",
    "\n",
    "# Replace NaN or non-string values with empty string\n",
    "data_v3['filtered_tweet'] = data_v3['filtered_tweet'].astype(str)\n",
    "cleaned_tweets = []\n",
    "\n",
    "for i in tqdm(range(len(data_v3['filtered_tweet']))):\n",
    "    tweet = data_v3['filtered_tweet'][i]\n",
    "    clean_text = p.clean(tweet)\n",
    "    filtered_tweet = clean_tweets(clean_text)\n",
    "    cleaned_tweets.append(filtered_tweet)\n",
    "    \n",
    "data_v3['filtered_tweet'] = cleaned_tweets\n",
    "\n",
    "data_v3 = data_v3[['filtered_tweet','is_depression']]\n",
    "data_v3.to_csv('../data/processed/Suicide_Detection_Processed_v3.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27977/27977 [01:36<00:00, 291.00it/s]\n"
     ]
    }
   ],
   "source": [
    "data_v4 = pd.read_csv('../data/raw/Mental_Health_Corpus.csv')\n",
    "data_v4.rename(columns = {'text':'filtered_tweet',\n",
    "                          'label':'is_depression'},\n",
    "                          inplace = True )\n",
    "cleaned_tweets = []\n",
    "\n",
    "for i in tqdm(range(len(data_v4['filtered_tweet']))):\n",
    "    tweet = data_v4['filtered_tweet'][i]\n",
    "    clean_text = p.clean(tweet)\n",
    "    filtered_tweet = clean_tweets(clean_text)\n",
    "    cleaned_tweets.append(filtered_tweet)\n",
    "    \n",
    "data_v4['filtered_tweet'] = cleaned_tweets\n",
    "data_v4.to_csv('../data/processed/Mental_Health_Corpus_Processed_v4.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10314/10314 [00:12<00:00, 801.86it/s]\n"
     ]
    }
   ],
   "source": [
    "data_v5 = pd.read_csv('../data/raw/Sentiment_Tweets.csv', usecols=['message to examine', 'label (depression result)'])\n",
    "data_v5.rename(columns = {\"message to examine\":'filtered_tweet',\n",
    "                          'label (depression result)':'is_depression'},\n",
    "                          inplace = True)\n",
    "cleaned_tweets = []\n",
    "\n",
    "for i in tqdm(range(len(data_v5['filtered_tweet']))):\n",
    "    tweet = data_v5['filtered_tweet'][i]\n",
    "    clean_text = p.clean(tweet)\n",
    "    filtered_tweet = clean_tweets(clean_text)\n",
    "    cleaned_tweets.append(filtered_tweet)\n",
    "    \n",
    "data_v5['filtered_tweet'] = cleaned_tweets\n",
    "data_v5.to_csv('../data/processed/Sentiment_Tweets_Processed_v5.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7731/7731 [00:17<00:00, 441.84it/s] \n"
     ]
    }
   ],
   "source": [
    "data_v6 = pd.read_csv('../data/raw/Depression_Dataset_Reddit.csv')\n",
    "data_v6.rename(columns = {\"clean_text\":'filtered_tweet',\n",
    "                          'is_depression':'is_depression'},\n",
    "                          inplace = True )\n",
    "cleaned_tweets = []\n",
    "for i in tqdm(range(len(data_v6['filtered_tweet']))):\n",
    "    tweet = data_v6['filtered_tweet'][i]\n",
    "    clean_text = p.clean(tweet)\n",
    "    filtered_tweet = clean_tweets(clean_text)\n",
    "    cleaned_tweets.append(filtered_tweet)\n",
    "    \n",
    "data_v6['filtered_tweet'] = cleaned_tweets\n",
    "data_v6.to_csv('../data/processed/Depression_Dataset_Reddit_Processed_v6.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346617, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_final = pd.concat([data_v1,\n",
    "                        data_v2,\n",
    "                        data_v3,\n",
    "                        data_v4,\n",
    "                        data_v5,\n",
    "                        data_v6])\n",
    "data_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(329775, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_final = pd.concat([data_v1,\n",
    "                        data_v2,\n",
    "                        data_v3,\n",
    "                        data_v4,\n",
    "                        data_v5,\n",
    "                        data_v6])\n",
    "data_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "is_depression\n",
      "0    168930\n",
      "1    160291\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final dataset shape: (329221, 2)\n"
     ]
    }
   ],
   "source": [
    "data_final = clean_and_validate_dataset(df = data_final, \n",
    "                           text_col = 'filtered_tweet', \n",
    "                          label_col = 'is_depression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the final data\n",
    "data_final.to_csv('../data/processed/data_final.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this data for modeling and get confident about the performance and good classifications models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
